{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 11:16:44.672179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-23 11:16:45.475171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "from src.config import CONFIG\n",
    "from src.dataset import SBICDataset\n",
    "from src.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all special token and its token_id:\n",
      " - ['<|endoftext|>', '<|sep|>', '<|pad|>', '<|offY|>', '<|offN|>', '<|sexY|>', '<|sexN|>', '<|intY|>', '<|intN|>', '<|grpY|>', '<|grpN|>', '<|ingrpN|>', '<|ingrpY|>']\n",
      " - [[50256], [50258], [50257], [50259], [50260], [50261], [50262], [50263], [50264], [50265], [50266], [50267], [50268]]\n",
      "Model vocab resize: 50269\n",
      "Model eos token: 50256\n",
      "Model pad token: 50257\n",
      "Model sep token: 50258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = make_tokinzer(CONFIG.hp)\n",
    "model = make_model(CONFIG.hp, tokenizer)\n",
    "checkpoints = \"gpt2_10832_8_2\"\n",
    "# checkpoints = \"distilgpt2_30720_16\"\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{checkpoints}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reference is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/matteo/github/social_bias_framework/test_models.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrouge\u001b[39;00m \u001b[39mimport\u001b[39;00m Rouge\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rouge_metric \u001b[39m=\u001b[39m Rouge()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m rouge_metric\u001b[39m.\u001b[39;49mget_scores(\u001b[39m\"\u001b[39;49m\u001b[39mciao\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/rouge/rouge.py:107\u001b[0m, in \u001b[0;36mRouge.get_scores\u001b[0;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mlen\u001b[39m(hyps) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(refs))\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m avg:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_scores(hyps, refs)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_avg_scores(hyps, refs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/rouge/rouge.py:120\u001b[0m, in \u001b[0;36mRouge._get_scores\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics:\n\u001b[1;32m    119\u001b[0m     fn \u001b[39m=\u001b[39m Rouge\u001b[39m.\u001b[39mAVAILABLE_METRICS[m]\n\u001b[0;32m--> 120\u001b[0m     sc \u001b[39m=\u001b[39m fn(\n\u001b[1;32m    121\u001b[0m         hyp,\n\u001b[1;32m    122\u001b[0m         ref,\n\u001b[1;32m    123\u001b[0m         raw_results\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_results,\n\u001b[1;32m    124\u001b[0m         exclusive\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclusive)\n\u001b[1;32m    125\u001b[0m     sen_score[m] \u001b[39m=\u001b[39m {s: sc[s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats}\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_lengths:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/rouge/rouge.py:53\u001b[0m, in \u001b[0;36mRouge.<lambda>\u001b[0;34m(hyp, ref, **k)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRouge\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     DEFAULT_METRICS \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mrouge-1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrouge-2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrouge-l\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m     AVAILABLE_METRICS \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge-1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk: rouge_score\u001b[39m.\u001b[39;49mrouge_n(hyp, ref, \u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk),\n\u001b[1;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge-2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk: rouge_score\u001b[39m.\u001b[39mrouge_n(hyp, ref, \u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk),\n\u001b[1;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge-3\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk: rouge_score\u001b[39m.\u001b[39mrouge_n(hyp, ref, \u001b[39m3\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk),\n\u001b[1;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge-4\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk: rouge_score\u001b[39m.\u001b[39mrouge_n(hyp, ref, \u001b[39m4\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk),\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge-5\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk: rouge_score\u001b[39m.\u001b[39mrouge_n(hyp, ref, \u001b[39m5\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk),\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge-l\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk:\n\u001b[1;32m     59\u001b[0m             rouge_score\u001b[39m.\u001b[39mrouge_l_summary_level(hyp, ref, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk),\n\u001b[1;32m     60\u001b[0m     }\n\u001b[1;32m     61\u001b[0m     DEFAULT_STATS \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m     AVAILABLE_STATS \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/rouge/rouge_score.py:255\u001b[0m, in \u001b[0;36mrouge_n\u001b[0;34m(evaluated_sentences, reference_sentences, n, raw_results, exclusive)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mHypothesis is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(reference_sentences) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mReference is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m evaluated_ngrams \u001b[39m=\u001b[39m _get_word_ngrams(\n\u001b[1;32m    258\u001b[0m     n, evaluated_sentences, exclusive\u001b[39m=\u001b[39mexclusive)\n\u001b[1;32m    259\u001b[0m reference_ngrams \u001b[39m=\u001b[39m _get_word_ngrams(\n\u001b[1;32m    260\u001b[0m     n, reference_sentences, exclusive\u001b[39m=\u001b[39mexclusive)\n",
      "\u001b[0;31mValueError\u001b[0m: Reference is empty."
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge_metric = Rouge()\n",
    "\n",
    "rouge_metric.get_scores(\"ciao\", \"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50259 50259 50259 50259 50259 50259 50259 50259]\n",
      "[50260 50260 50260 50259 50259 50259 50259 50259]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/matteo/github/social_bias_framework/test_models.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m SBICDataset(data, tokenizer, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m make_dataloader(dataset, model, tokenizer, CONFIG\u001b[39m.\u001b[39mhp, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m res \u001b[39m=\u001b[39m evaluate(model, tokenizer, dataloader, CONFIG\u001b[39m.\u001b[39;49mhp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m annotation_type \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mOffensive\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mIntent\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSex\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGroup\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mIn-Group\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matteo/github/social_bias_framework/test_models.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClassification F1 on \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m set: avg=\u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(res[\u001b[39m'\u001b[39m\u001b[39mclasification_f1_score\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/social_bias_framework/src/train_utils.py:276\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer, dataloader, config)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m# remove from the output the input prompt\u001b[39;00m\n\u001b[1;32m    274\u001b[0m preds \u001b[39m=\u001b[39m [gen[np\u001b[39m.\u001b[39mwhere(gen \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39msep_token_id)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m gen \u001b[39min\u001b[39;00m generate_out]\n\u001b[0;32m--> 276\u001b[0m score \u001b[39m=\u001b[39m evaluate_predictions(tokenizer, preds, data)\n\u001b[1;32m    277\u001b[0m class_f1s\u001b[39m.\u001b[39mappend(score[\u001b[39m\"\u001b[39m\u001b[39mclass_f1\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    278\u001b[0m minor_rouge\u001b[39m.\u001b[39mextend(score[\u001b[39m\"\u001b[39m\u001b[39mrouge_minor\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/github/social_bias_framework/src/evaluation.py:13\u001b[0m, in \u001b[0;36mevaluate_predictions\u001b[0;34m(tokenizer, predictions, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m class_labels, minority_labels, stereotype_labels \u001b[39m=\u001b[39m get_labels(tokenizer, labels)\n\u001b[1;32m     11\u001b[0m class_preds, minority_preds, stereotype_preds \u001b[39m=\u001b[39m get_predictions(tokenizer, predictions)\n\u001b[0;32m---> 13\u001b[0m f1_classifiaction \u001b[39m=\u001b[39m eval_classification_tokens(tokenizer, class_labels, class_preds)\n\u001b[1;32m     14\u001b[0m f1_rouge_minority \u001b[39m=\u001b[39m eval_generation_tokens(minority_labels, minority_preds)\n\u001b[1;32m     15\u001b[0m f1_rouge_stereotype \u001b[39m=\u001b[39m eval_generation_tokens(stereotype_labels, stereotype_preds)\n",
      "File \u001b[0;32m~/github/social_bias_framework/src/evaluation.py:103\u001b[0m, in \u001b[0;36meval_classification_tokens\u001b[0;34m(tokenizer, labels, predictions)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mprint\u001b[39m(preds)\n\u001b[1;32m    102\u001b[0m \u001b[39mprint\u001b[39m(good_idx)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    104\u001b[0m f1 \u001b[39m=\u001b[39m f1_score([lbls[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m good_idx],\n\u001b[1;32m    105\u001b[0m               [preds[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m good_idx],\n\u001b[1;32m    106\u001b[0m                zero_division\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    107\u001b[0m                average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m f1_scores\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mnan_to_num(f1))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "n_samples = {\n",
    "    \"train\": 1024,\n",
    "    \"validation\": 1024\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    data = get_data(split)[:n_samples[split]]\n",
    "    dataset = SBICDataset(data, tokenizer, is_training=False)\n",
    "    dataloader = make_dataloader(dataset, model, tokenizer, CONFIG.hp, split=\"validation\")\n",
    "\n",
    "    res = evaluate(model, tokenizer, dataloader, CONFIG.hp)\n",
    "\n",
    "    annotation_type = [\"Offensive\", \"Intent\", \"Sex\", \"Group\", \"In-Group\"]\n",
    "    print(f\"Classification F1 on {split} set: avg={np.mean(res['clasification_f1_score']):.3f}\")\n",
    "    for type, score in zip(annotation_type, res['clasification_f1_score']):\n",
    "        print(f\" - {type}: {score:.3f}\")\n",
    "    print(f\"Minority RougeL-f1 on {split} set: {res['minority_rouge_f1_score']:.3f}\")\n",
    "    print(f\"Stereotype RougeL-f1 on {split} set: {res['stereotype_rouge_f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification F1 on train set: avg=0.590\n",
    " - Offensive: 0.577\n",
    " - Intent: 0.508\n",
    " - Sex: 0.787\n",
    " - Group: 0.425\n",
    " - In-Group: 0.652\n",
    "Minority RougeL-f1 on train set: 0.690\n",
    "Stereotype RougeL-f1 on train set: 0.336\n",
    "                                                 \n",
    "Classification F1 on validation set: avg=0.578\n",
    " - Offensive: 0.579\n",
    " - Intent: 0.490\n",
    " - Sex: 0.790\n",
    " - Group: 0.453\n",
    " - In-Group: 0.575\n",
    "Minority RougeL-f1 on validation set: 0.519\n",
    "Stereotype RougeL-f1 on validation set: 0.243"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
